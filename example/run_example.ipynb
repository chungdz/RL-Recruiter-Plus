{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running Example of RL-Recruiter+\n",
    "\n",
    "This jupyter notebook displays how to build and train the RL-Recruiter+ and then do the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "import numpy as np\n",
    "sys.path.append('../')\n",
    "from rl_recruiter.rl_model import RL_Recruiter_plus\n",
    "from rl_recruiter.entropy_cal import type_2_entro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to obtain the predictability of each participants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_2_entro('./data/trajectory.json', './data/predictability.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We build the RL-Recruiter using two parameter settings files.\n",
    "\n",
    "The first settings saved in a json file that needs to initialize the model. A example is in this [page](https://github.com/chungdz/RL-Recruiter-Plus/blob/master/example/data/para_settings.json).\n",
    "\n",
    "There are several parameters need to be deployed:\n",
    "* \"total_person\": the number of whole participants.\n",
    "* \"max_user\": the number of participants to be selected.\n",
    "* \"train_start\": the index in the list of trajectories that the model needs to begin learning.\n",
    "* \"train_end\": the index in the list of trajectories that the model needs to stop learning. For example, if the trajectories are from 10 days and each day's trajectories are gathered in a list, we set \"train_start\" as 0 and \"train_end\" as 10.\n",
    "* \"train_epoch\": the number of training epoch.\n",
    "* \"layer\": the number of rows in value function table, cannot be higher than \"max_user\", the higher the layer, the larger the table, and thus it needs more data and epoch to learn and may get a better result.\n",
    "\n",
    "There is another file needs to be imported to the model, which contains the threshold values for discretizing the entropy values which is one input to do the training and predicting. It is in the json list format. Just using our [thresholds](https://github.com/chungdz/RL-Recruiter-Plus/blob/master/example/data/thres.json) is fine. There are 100 threshold in the list. Making another file with the same format is also feasible. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "rlp = RL_Recruiter_plus('./data/para_settings.json', './data/thres.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Process\n",
    "### Input\n",
    "There are two data files need to be input. \n",
    "\n",
    "The first is the trajectory data, you can see example [here](https://github.com/chungdz/RL-Recruiter-Plus/blob/master/example/data/trajectory.json). The trajectory data is a dictionary saved in json format. The key is a participant id and the value is a list of trajectory sets in each time period. The participant ids need to be mapped into consecutive integer. If there are 100 participants, then the key list in trajectory dictionary are like [\"0\", \"1\", ..., \"99\"]. One trajectory set contains nonredundant categorical integers representing the area covered by its participant in this time period. The integer \"-1\" in trajectory sets will be ignored and not counted in coverage.\n",
    "\n",
    "The other is the participants' predictability data that can be calculated from trajectory data. The format of the predictability data is also a dictionary with keys the participant ids and values the list of entroy values in each time period.\n",
    "### Output\n",
    "RL-Recruiter+ shows the training results for each time period by select participants and calculate their absolute coverage and reletive coverage at the beginning of the next time period(e.g. using the trained value function after the j-1 time period to select participants in the beginning of the j time period to show performance). \n",
    "\n",
    "Absolute coverage is the exact number of area covered. Relative coverage is the absolute coverage divided by the highest possible coverage, which is obtained by selecting all participants. \n",
    "\n",
    "In the first time period the RL-Recruiter+ randomly selects participants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading trajectory Data...\n",
      "Getting eligible user list\n",
      "init record set\n",
      "load entro_daily\n",
      "train and evaluate from day to day\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 14/14 [03:03<00:00, 13.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Each day's coverage:\n",
      "[664, 778, 1115, 1191, 1267, 954, 1268, 1323, 1489, 1259, 1219, 1349, 1082, 1055]\n",
      "relative coverage:\n",
      "[6628, 6529, 6530, 6531, 6532, 6533, 6534, 6434, 6435, 6335, 6235, 6135, 6035, 5935, 5835, 5735, 5736, 5635, 5536, 5436, 5336, 5236, 5136, 5036, 4936, 7600, 7601, 7602, 7603, 7604, 7504, 7404, 7304, 7305, 7205, 7206, 7106, 7006, 6906, 6806, 6807, 6808, 6809, 6810, 6910, 6911, 6912, 6913, 6914, 6814, 6815, 6816, 6817, 6818, 6719, 6619, 6519, 6419, 6319, 6219, 6119, 6019, 5919, 5920, 5921, 5922, 5923, 5924, 5925, 5926, 5927, 5928, 5828, 5728, 5628, 5528, 5428, 5429, 5329, 5229, 5129, 5029, 4929, 4930, 4931, 4932, 4933, 4934, 4935, 4937, 4938, 4839, 4840, 4740, 4640, 4540, 4440, 4340, 4341, 4241, 4240, 4141, 4041, 3941, 3841, 3741, 3641, 3541, 3441, 3341, 3241, 3141, 3041, 2941, 2942, 2841, 2741, 2641, 2541, 2542, 2442, 2441, 2342, 2341, 2242, 2241, 2142, 2042, 1942, 1842, 1742, 1642, 1542, 1442, 1342, 1242, 1142, 1042, 942, 842, 742, 741, 740, 739, 839, 939, 838, 738, 637, 638, 941, 1040, 940, 840, 639, 640, 641, 642, 6524, 6424, 6425, 6525, 6426, 6427, 6328, 6228, 6227, 6128, 6028, 5328, 5228, 5128, 4939, 4940, 4941, 4942, 4943, 5043, 5044, 5042, 5135, 5235, 5335, 5435, 5535, 6334, 6333, 6332, 6331, 6330, 6329, 6428, 7130, 7129, 7030, 6931, 6932, 6933, 6834, 6835, 6735, 6635, 6535, 5936, 5836, 5636, 5237, 5238, 5239, 5240, 5241, 5242, 5142, 6934, 7031, 5437, 5438, 5439, 5440, 5441, 5341, 5143, 4841, 4741, 4641, 4541, 4441, 4140, 4040, 3940, 3840, 3740, 3739, 3639, 3539, 3440, 3339, 3338, 3337, 3336, 3335, 3334, 3234, 3233, 3232, 3231, 3230, 3229, 3228, 3227, 3226, 3325, 3519, 3619, 3521, 3622, 3522, 3523, 3524, 3624, 3525, 3625, 3626, 3726, 3826, 3925, 3926, 3927, 3928, 3929, 3930, 4031, 4131, 4231, 4232, 4233, 4234, 4235, 4236, 4237, 4238, 4239, 4339, 4842, 5538, 5638, 5738, 5737, 5734, 5733, 5732, 5632, 5631, 5630, 5629, 6018, 6017, 6117, 6116, 6216, 6115, 6015, 6014, 6114, 6113, 6013, 6012, 6016, 5916, 1274, 1275, 1175, 1176, 1177, 1273, 1372, 1373, 1276, 1277, 1278, 874, 978, 979, 879, 878, 778, 678, 679, 579, 578, 577, 576, 675, 674, 574, 474, 374, 373, 273, 274, 173, 172, 71, 171, 70, 69, 68, 67, 66, 64, 63, 61, 160, 260, 361, 462, 562, 664, 666, 765, 565, 458, 457, 459, 460, 358, 359, 357, 356, 254, 251, 150, 250, 351, 452, 454, 555, 556, 553, 551, 550, 548, 547, 545, 644, 643, 539, 537, 536, 435, 434, 332, 231, 230, 23, 1052, 1051, 1151, 1152, 1150, 1149, 1148, 1147, 1047, 1046, 1146, 1045, 1145, 1245, 1246, 1346, 1347, 1447, 1448, 1446, 1345, 1244, 1243, 1343, 1443, 1643, 1843, 1844, 1945, 1946, 1947, 1948, 1949, 1950, 1951, 1952, 1953, 1954, 1955, 1956, 1957, 1958, 1959, 1960, 1961, 1962, 1963, 1964, 1965, 1966, 1967, 1968, 1969, 1970, 1971, 1972, 1973, 1974, 1975, 1976, 2076, 2077, 2078, 2079, 1979, 1980, 2080, 1981, 1982, 1983, 2083, 2084, 2085, 2086, 2087, 2088, 2089, 2090, 2091, 2092, 2093, 2094, 2095, 1996, 1997, 2098, 2099, 5584, 5684, 5583, 5683, 5682, 5582, 5482, 5382, 5282, 5281, 5181, 5081, 4981, 4980, 4979, 4978, 4977, 4976, 4975, 4974, 4973, 4972, 4971, 4970, 4969, 4968, 4967, 4966, 4865, 4864, 4863, 4862, 4861, 4860, 4859, 4858, 4857, 4856, 4855, 4854, 4754, 4654, 4653, 4553, 4552, 4452, 4451, 4450, 4349, 4348, 4248, 4247, 4246, 4146, 4145, 4045, 3744, 3844, 3944, 4044, 3743, 3643, 3543, 3945, 4046, 4245, 4144, 4043, 4244, 4346, 4345, 4445, 4142, 4143, 4243, 4242, 4042, 3942, 3130, 3129, 3028, 2927, 2926, 2826, 2825, 2724, 2723, 2622, 2621, 2521, 2520, 2419, 2418, 2317, 2216, 2215, 2115, 2114, 2013, 2012, 1912, 1911, 1910, 1810, 1809, 1808, 1707, 1606, 1605, 1504, 1503, 1402, 1401, 1300, 4147, 4148, 4048, 4051, 3952, 4543, 4443, 4444, 3950, 3949, 59, 159, 158, 258, 557, 657, 757, 857, 956, 1056, 1256, 1356, 1456, 1556, 1655, 1855, 2055, 2054, 2154, 2254, 2354, 2454, 2554, 2654, 2754, 2854, 2954, 3054, 3053, 3153, 3253, 3353, 3453, 3252, 3251, 3250, 3249, 3248, 3247, 3246, 3245, 3244, 95, 195, 295, 294, 394, 395, 495, 496, 596, 595, 594, 695, 795, 794, 894, 994, 1094, 1194, 1294, 1394, 1494, 1594, 1595, 1696, 1697, 1698, 1797, 1798, 1799, 94, 93, 92, 1941, 1940, 1939, 1938, 1937, 1936, 1935, 1934, 1933, 1932, 1931, 1930, 1929, 1928, 2028, 2027, 2127, 2227, 2327, 2427, 2426, 2526, 2626, 2625, 2624, 2623, 2620, 2619, 2618, 2617, 2616, 2515, 2514, 2614, 2615, 2715, 2716, 2815, 2816, 2613, 2513, 1445, 1543, 1644, 1744, 1944, 2043, 2143, 2243, 2244, 2245, 2246, 2146, 2147, 2248, 2249, 2250, 2251, 2252, 2353, 2453, 2553, 2653, 2853, 2953, 3052, 3152, 3352, 3351, 2549, 2649, 2650, 2750, 3354, 3454, 91, 90, 89, 88, 87, 86, 85, 84, 83, 82, 65, 62, 60, 58, 57, 56, 55, 54, 53, 52, 51, 153, 154, 354, 353, 453, 779, 980, 981, 982, 1082, 1083, 983, 984, 985, 986, 987, 1087, 1088, 1089, 1189, 1190, 558, 259, 658, 659, 758, 955, 1055, 1054, 1154, 1253, 1453, 1353, 1354, 1254, 257, 1157, 957, 1756, 161, 261, 262, 360, 157, 560, 660, 760, 43, 143, 243, 343, 443, 543, 743, 1141, 1241, 1341, 1441, 1440, 1541, 1641, 1841, 1927, 1926, 1925, 1924, 2024, 1923, 542, 442, 44, 45, 46, 47, 48, 49, 50, 152, 151, 651, 650, 750, 749, 450, 451, 1043, 1143, 1743, 2041, 2040, 1922, 1921, 1920, 1919, 1918, 1917, 1916, 1915, 1914, 1913, 1909, 1908, 1907, 1906, 1905, 1805, 1904, 1903, 1902, 1901, 1900, 1800, 1700, 1600, 1500, 1400, 1200, 1100, 1000, 900, 901, 801, 701, 601, 501, 401, 301, 201, 202, 102, 2, 3, 31, 32, 33, 34, 35, 37, 38, 39, 40, 41, 1001, 800, 700, 600, 500, 302, 28, 29, 30, 36, 42, 6029, 5929, 3640, 3340, 3342, 3242, 3243, 3343, 3344, 3347, 3346, 3348, 3349, 3350, 3255, 3256, 3356, 3357, 3358, 3359, 3259, 3360, 3260, 3261, 3262, 3263, 3363, 3364, 3365, 3366, 3367, 3368, 3369, 3370, 3371, 3372, 3373, 3374, 3375, 3376, 3377, 3378, 3379, 3380, 3381, 3382, 3383, 3384, 3385, 3386, 3387, 3388, 3389, 3390, 3391, 3392, 3292, 3192, 3092, 2992, 2892, 2792, 2692, 2592, 2492, 2493, 2393, 2392, 2292, 2293, 2193, 2096, 2097, 2199, 2198, 2197, 2196, 2195, 2194, 2294, 2593, 3191, 3291, 3485, 3484, 3483, 3482, 3481, 3265, 3264, 3361, 3258, 3355, 3254, 81, 80, 253, 552, 656, 554, 455, 355, 352, 252, 6725, 6824, 6924, 6925, 6825, 6826, 6926, 6927, 6928, 6929, 7029, 7132, 6829, 6728, 6828, 6827, 2343, 2443, 2543, 2642, 2742, 2842, 3042, 3446, 3362, 3270, 3271, 3488, 3489, 3491, 2192, 1993, 1893, 1793, 1693, 1593, 1493, 1393, 1293, 1193, 1093, 694, 494, 194, 493, 593, 693, 793, 792, 892, 893, 896, 897, 898, 899, 1099, 1098, 1097, 1095, 2693, 2793, 2893, 3490, 3487, 3486, 3269, 3268, 3267, 3266, 3257, 3345, 3142, 2141, 1344, 1144, 943, 2283, 1140, 841, 744, 4742, 5039, 5040, 5041, 5140, 5141, 5139, 5035, 6036, 6935, 6930, 6727, 7025, 7125, 7126, 7127, 7128, 7131, 7133, 7134, 7135, 7136, 7137, 7037, 7038, 6938, 6939, 6940, 6840, 6841, 6741, 6742, 6743, 6842, 4739, 4639, 4539, 4439, 4139, 4039, 3939, 3839, 843, 244, 144, 145, 146, 1777, 1676, 1675, 1574, 1818, 1819, 1820, 1821, 1822, 1823, 1824, 1825, 1826, 1836, 1943, 2144, 2344, 2444, 2544, 2643, 2743, 3143, 3442, 3542, 3642, 3742, 3842, 4342, 4442, 4542, 4642, 6226, 6225, 6126, 4996, 4997, 4998, 4999, 2038, 2035, 2034, 2030, 2029, 2128, 2527, 2627, 2714, 2915, 3015, 3014, 3013, 3113, 3213, 3214, 3313, 3412, 3413, 3513, 3512, 3612, 3712, 3812, 3912, 4012, 4112, 4111, 4110, 4210, 4211, 4212, 4213, 4214, 4314, 4414, 4514, 4614, 4714, 4814, 4914, 5013, 5113, 5213, 5313, 5413, 5514, 5614, 5714, 5813, 5913, 5914, 5915, 5917, 5918, 5816, 5716, 5717, 5617, 5517, 5518, 5519, 5419, 5320, 5321, 5221, 5122, 5121, 5120, 5020, 4920, 4820, 4720, 4620, 4619, 4519, 4419, 4421, 4420, 4319, 4219, 4218, 4217, 4216, 4215, 4011, 3312, 3114, 9591, 8898, 2189, 2183, 2073, 2070, 2066, 2061, 2056, 2051, 2047, 344, 345, 346, 347, 348, 349, 350, 2191, 2190, 2188, 2187, 2186, 2185, 2184, 2182, 2181, 2180, 2179, 2178, 2177, 2176, 2175, 2174, 2173, 2172, 2171, 2170, 2169, 2168, 2167, 2166, 2165, 2164, 2163, 2162, 2161, 1861, 1761, 1661, 1561, 1461, 1361, 1261, 1161, 1061, 1062, 962, 862, 762, 662, 463, 363, 364, 264, 165, 166, 167, 4, 103, 203, 300, 902, 1101, 1002, 1102, 1004, 1103, 1501, 1601, 1701, 1801, 2001, 2100, 2200, 2300, 2400, 2500, 2600, 2700, 2800, 2900, 3000, 3219, 3218, 3118, 3217, 3315, 3216, 3117, 3018, 3119, 3019, 3016, 3320, 3220, 3120, 3221, 3222, 3322, 3321, 3215, 3212, 3110, 3109, 3107, 3105, 3103, 3101, 3100, 3200, 2000, 400, 200, 100, 1, 5125, 3438, 3239, 3240, 3154, 2753, 1755, 1555, 1455, 1156, 3554, 3553, 3653, 3753, 3853, 3953, 4053, 4153, 4253, 4353, 4453, 4753, 4853, 4953, 5053, 5153, 5253, 5353, 5453, 5553, 5652, 5752, 5852, 5952, 6052, 6152, 6252, 6352, 6351, 6451, 6551, 6550, 6650, 6649, 6749, 6848, 6847, 6946, 6945, 7045, 7044, 7143, 7142, 7242, 7241, 7340, 7339, 7439, 7438, 7537, 7536, 7636, 7735, 7734, 7833, 7932, 7931, 8030, 8129, 8128, 8228, 8227, 8326, 8325, 8425, 8424, 8524, 8623, 8723, 8722, 8822, 8922, 9021, 9121, 9120, 9220, 9320, 9319, 9419, 9519, 9518, 9618, 9718, 9817, 9917, 9916, 9717, 8921, 8523, 8226, 8029, 8031, 7832, 7635, 6846, 6748, 5552, 5452, 5352, 5252, 1057, 1301]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "rlp.train_and_evaluate('./data/trajectory.json', './data/predictability.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction Process\n",
    "\n",
    "There is an optional input to do the prediction. This input is a list of entropy values with the length equal to number of total participants. Value in dimension k represents the entropy value for participant with id \"k\". \n",
    "\n",
    "The RL_Recruiter+ gives a list of promoting participants after the predict process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First get the entropy list.(optional)\n",
    "total_person = rlp.hypara_dict['total_person']\n",
    "train_last_day = rlp.hypara_dict['train_end']\n",
    "\n",
    "with open('./data/predictability.json', 'r') as f:\n",
    "    entro_dict = json.load(f)\n",
    "\n",
    "last_day = len(entro_dict[\"0\"]) - 1\n",
    "\n",
    "entro_list = np.zeros((total_person))\n",
    "for k, v in entro_dict.items():\n",
    "    try:\n",
    "        entro_list[int(k)] = v[train_last_day - 1]\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[127, 4, 1, 69, 9, 8, 129, 170, 167, 85, 152, 12, 96, 145, 42, 7, 128, 93, 5, 32]\n"
     ]
    }
   ],
   "source": [
    "# Then do the prediction.\n",
    "selected_participants = rlp.predict(entro_list=entro_list)\n",
    "print(selected_participants)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
